---
layout: post
title: Nginx性能
key: 20180223
tags: Nginx
---

## Nginx性能

### tcp_nodelay,tcp_nopush和sendfile

#### tcp_nodelay

>在TCP发展早期，工程师需要面对流量冲突和堵塞的问题，其中涌现了大批的解决方案，其中之一是由John Nagle提出的算法。

>Nagle的算法旨在防止通信被大量的小包淹没，该理论不涉及全尺寸的tcp包(最大报文长度，简称MSS)的处理，只针对比MSS小的包。只有当接收方成功地将以前的包(ACK)的所有确认发回来时，这些包才会被发送。在等待期间，发送方可以缓冲更多的数据之后在发送。
<pre>
if package.size >= MSS.szie
   send(package)
elsif acks.all_received?
   send(package)
else
   # acumulate date
end
</pre>

>与此同时，诞生了另外一个理论，延时ACK

>在TCP通信中，在发送数据后，需要接受回应包(ACK)来确认数据被成功传达。

>延时ACK旨在解决线路被大量的ACK包拥堵的状况。为了减少ACK的数量，接受者等待需要回传的数据加上ACK包回传给发送方，如果没有数据需要回传，必须在至少每2个MSS，或者200至500毫秒内发送ACK(以防我们不在收到ACK包)。

<pre>
if packages.any?
  send
elsif last_ack_send_more_then_2MSS_ago? || 200_ms_500_timer.finished?
  send
else
  # wait
end
</pre>
>正如可能在一开始就注意到的那样————这可能会导致持久连接上的一些暂时的锁死。让我们重视它。

假设：

- 初始拥塞窗口等于2，拥塞窗口是另一个TCP机制的一部分，称为慢启动。细节并不重要，只要记住它限制了一次可以发送多少个包。在第一次往返中，我们可以发送2个MSS包。在第二次发送中：4个MSS包，第三次发送8个MSS包，以此类推。
- 四个已缓存的等待发送的数据包：A,B,C,D
- A,B,C是MSS包
- D是小包

场景：

- 由于是初始的拥塞窗口，发送端被允许传送两个包：A和B

- 接收端在成功获得这两个包之后，发送一个ACK

- 发送端发送C包。软后，Nagle却阻止它发送D包(包长度太小，等待C包的ACK)

- 在接收端，延时ACK使他无法发送ACK(每隔2个包或者200毫秒发送一次)

- 在200ms之后，接收端发送C包的ACK

- 发送端收到ACK并发送D包

如图所示：

![nginxtcp](https://blog.maihill.com/assets/images/pic/nginx/TCP%E5%BB%B6%E8%BF%9F%E7%AD%96%E7%95%A5.jpg "nginxtcp")

>在这个数据交换过程中，由于Nagle和延时ACK之间的死锁，引入了200毫秒的延迟

>Nagle算法是当时正真的救世主，而且目前任然具有极大的价值。但在大多数情况下，我们不会在我们的网站上使用它，因此可以通过添加TCP_NODELAY标志来安全的关闭它。来享受200毫秒的提速效果。
<pre>
tcp_nodelay on; # sets TCP_NODELAY flag, used on keep-alive connections
</pre>


#### sendfile
正常来说，党要发送一个文件时需要下面的步骤：

- malloc(3)——分配一个本地缓冲区，储存对象数据

- read(2)——检索和复制对象到本地缓冲区

- write(2)——从本地缓冲区复制对象到socket缓冲区

>这涉及到两个上下文切换(读、写)，并使相同对象的第二个副本成为不必要的。正如你所看到的，这不是最佳方式。值得庆幸的是还有另一个系统调用，提升了发送文件(的效率)，它北称为：sendfile(2).这个调用在文件cache中检索一个对象，并传递指针(不需要复制整个对象)，直接传递到socket描述符，Netflix表示，使用sendfile(2)将网络吞吐量从6Gbps提高到30Gbps。

sendfile(2)有一些注意事项：

- 不可用于UNIX socket(当通过你的上游服务器发送静态文件时)

- 能否执行不同的操作，取决于操作系统

在Nginx中打开sendfile(2)的方法：
<pre>
sendfile on;
</pre>

#### tcp_nopush

>tcp_nopush和tcp_nodelay相反。不是为了尽可能快的推送数据包，它的目标是一次性优化数据的发送量。

>在发送客户端之前，它将强制等待包达到最大长度(MSS),而且这个指令只在sendfile开启时才起作用。

设置方法如下：

<pre>
sendfile on;
tcp_nopush on;
</pre>

看起来tcp_nopush和tcp_delay是互斥的，但是，如果所有三个指令都开启，Nginx会怎样呢？

- 确保数据包在发送给客户端之前是满的

- 对于最后一个数据包，tcp_nopush将被删除，允许TCP立即发送，没有200毫秒的延迟

### 到底应该使用多少个进程呢？

#### 工作进程

>worker_process指令会指定：应该运行都少个worker。默认情况下此值为1，最安全的设置是通过传递auto选项来使用核心数。

>但由于Nginx架构，其处理请求的速度非常快，我们可能一次不会使用超过2-4个进程(除非你在托管很大的网站如百度等或者Nginx内部在执行一些CPU密集型任务)。

设置方法：

<pre>
worker_process auto;
</pre>

#### worker连接
>与worker_process直接绑定的指令时worker_connections.它指定一个工作进程可以一次打开多少个连接，这个数目包括所有连接(与代理服务器的连接)，而不仅仅是与客户端的连接，此外，值得记住的是，一个客户端可以打开多少个连接，同时获取其他资源。

设置方法：
<pre>
worker_connections 1024;
</pre>

#### 打开文件数目限制

>在基于Unix系统中的"一切都是文件"，这以为着文档，目录，管道甚至套接字都是文件。系统对一个进程可以打开多少个文件有一个限制，查看该限制：

<pre>
ulimit -Sn # soft ulimit
ulimit -Sn # hard ulimit
</pre>
>这个系统必须根据worker_connections进行调整，任何传入的连接都会打开至少一个文件(通常是两个套接字或者磁盘上的静态文件)。所以这个值等于worker_connections * 2是安全的。幸运的是，Nginx提供了一个配置选项来增加这个系统值，要使用这个配置，请添加具有适当数目的worker_rlimit_nofile指令并重新加载Nginx。

设置方法：
<pre>
worker_rlimit_nofile 2048;
</pre>

完整详细配置：
<pre>
worker_process auto;
worker_rlimit_nofile 2048; # Changes the limit on the maximun munber of open file(RLIMIT_NOFILE)for worker processes
worker_connections 1024; # Sets the maximnu mumber of simultaneous connections that can be opened by a worker processes
</pre>

#### 最大连接数

可以计算出可以处理多少给并发：
<pre>
最大连接数 =
   worker_process * worker_connections

   (keep_alive_timeout + avg_response_time) / 2
</pre>

>keep_alive_timeout + avg_response_time 单个连接持续多久，除以2，通常情况下一个客户端打开两个连接，一个Nginx和客户端之间，另一个在Nginx和上游服务器之间。

### Gzip
启用gzip可以显著降低响应的(报文)大小，因此，客户端(网页)会显得更快些
#### 压缩级别
>Gzip有不同的压缩级别，1 到 9 级。递增这个级别将会减少文件的大小，但也会增加资源消耗。作为标准将这个数字（级别）保持在 3 - 5 级，就像上面说的那样，它将会得到较小的节省，同时也会得到更大的 CPU 使用率。这有个通过 gzip 的不同的压缩级别压缩文件的例子，0 代表未压缩文件。
#### gzip_http_version 1.1;
>这条指令告诉nginx仅在HTTP 1.1 以上的版本才能使用 gzip,在这里不涉及 HTTP 1.0，至于 HTTP 1.0 版本，它是不可能既使用keep-alive和gzip的。因此你必须做出决定：使用 HTTP 1.0 的客户端要么错过 gzip，要么错过 keep-alive